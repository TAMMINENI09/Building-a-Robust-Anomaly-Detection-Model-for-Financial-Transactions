# -*- coding: utf-8 -*-
"""Final_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JKaWl_uABAbhmX5fnpMo3F908VW1Ct9z
"""

import pandas as pd
import numpy as np
! pip install torch_geometric
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings
# Ignore all warnings
warnings.filterwarnings("ignore")

data=pd.read_csv(r"/content/transaction_anomalies_dataset.csv")

data

# @title Frequency_of_Transactions

from matplotlib import pyplot as plt
data['Frequency_of_Transactions'].plot(kind='hist', bins=20, title='Frequency_of_Transactions')
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title Day_of_Week

from matplotlib import pyplot as plt
import seaborn as sns
data.groupby('Day_of_Week').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# Examine the dataset's structure, feature names, and data types
print("Dataset Structure:")
print(data.info())

# Calculate summary statistics for numerical features
print("\nSummary Statistics for Numerical Features:")
print(data.describe())

# Identify any missing values and understand their patterns
print("\nMissing Values:")
print(data.isnull().sum())

# Data Preprocessing

# Define numerical and categorical features
numerical_features = ['Transaction_Amount', 'Transaction_Volume', 'Average_Transaction_Amount',
                      'Frequency_of_Transactions', 'Time_Since_Last_Transaction', 'Age', 'Income']
categorical_features = ['Gender', 'Account_Type']

# Define preprocessing steps for numerical and categorical features
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with mean
    ('scaler', StandardScaler())  # Normalize features to ensure uniformity
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent value
    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Encode categorical variables using one-hot encoding
])

# Combine preprocessing steps for numerical and categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Apply preprocessing steps to the dataset
processed_data = preprocessor.fit_transform(data)

# Convert processed_data to a DataFrame for further analysis (optional)
processed_data = pd.DataFrame(processed_data)

# Display the processed DataFrame
print(processed_data.head())

# Feature Engineering

# Import necessary libraries
from sklearn.decomposition import PCA

# Consider additional feature creation, transformations, and aggregations as needed
# For example, let's create a new feature representing the ratio of Transaction_Amount to Average_Transaction_Amount
data['Amount_Ratio'] = data['Transaction_Amount'] / data['Average_Transaction_Amount']

# Apply PCA for dimensionality reduction if necessary
# For demonstration purposes, let's assume we want to reduce numerical features to 2 principal components
pca = PCA(n_components=2)
pca_features = pca.fit_transform(data[numerical_features])  # assuming numerical_features are already defined

# Concatenate PCA features with existing features
data_pca = pd.DataFrame(pca_features, columns=['PCA1', 'PCA2'])
data_combined = pd.concat([data, data_pca], axis=1)

# Display the dataset with engineered features
print(data_combined.head())

print(data.columns)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline


# Define features and target
X = data.drop(columns=['Transaction_ID', 'Account_Type'])  # Exclude Transaction_ID and Account_Type from features
y = data['Account_Type']

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define preprocessing steps for numerical and categorical features
numerical_features = ['Transaction_Amount', 'Transaction_Volume', 'Average_Transaction_Amount',
                      'Frequency_of_Transactions', 'Time_Since_Last_Transaction', 'Age', 'Income']
categorical_features = ['Gender', 'Day_of_Week', 'Time_of_Day']

numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps for numerical and categorical features
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Fit and transform the preprocessing steps on the training data
X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

from sklearn.ensemble import IsolationForest

# Initialize Isolation Forest model
isolation_forest = IsolationForest(random_state=42)

# Train the model
isolation_forest.fit(X_train_processed)

# Step 2: Model Selection

# Import necessary libraries
from sklearn.ensemble import IsolationForest
from sklearn.metrics import classification_report

# Initialize anomaly detection model
anomaly_model = IsolationForest()

# Train the anomaly detection model
anomaly_model.fit(X_train_processed)

# Predict anomalies on the training set
y_pred_train = anomaly_model.predict(X_train_processed)

# Convert true labels to binary labels
y_train_binary = [1 if label == 'Savings' else 0 if label == 'Current' else -1 for label in y_train]

# Evaluate model performance on training set
print("Training Set Evaluation:")
print(classification_report(y_train_binary, y_pred_train))

# Predict anomalies on the testing set
y_pred_test = anomaly_model.predict(X_test_processed)

# Convert true labels to binary labels for testing set
y_test_binary = [1 if label == 'Savings' else 0 if label == 'Current' else -1 for label in y_test]

# Evaluate model performance on testing set
print("Testing Set Evaluation:")
print(classification_report(y_test_binary, y_pred_test))

from sklearn.metrics import accuracy_score

# Calculate accuracy on training set
train_accuracy = accuracy_score(y_train_binary, y_pred_train)
print("Training Set Accuracy:", train_accuracy)

# Calculate accuracy on testing set
test_accuracy = accuracy_score(y_test_binary, y_pred_test)
print("Testing Set Accuracy:", test_accuracy)

from sklearn.svm import OneClassSVM
from sklearn.metrics import classification_report

# Initialize One-Class SVM model
svm_model = OneClassSVM()

# Train One-Class SVM model
svm_model.fit(X_train_processed)

# Predict anomalies on training and testing sets
y_pred_train_svm = svm_model.predict(X_train_processed)
y_pred_test_svm = svm_model.predict(X_test_processed)

# Convert predictions to binary labels
y_pred_train_binary_svm = [1 if x == -1 else 0 for x in y_pred_train_svm]
y_pred_test_binary_svm = [1 if x == -1 else 0 for x in y_pred_test_svm]

# Evaluate model performance on training set
print("Training Set Evaluation:")
print(classification_report(y_train_binary, y_pred_train_binary_svm))

# Evaluate model performance on testing set
print("Testing Set Evaluation:")
print(classification_report(y_test_binary, y_pred_test_binary_svm))

from sklearn.metrics import accuracy_score

# Calculate accuracy on the training set
training_accuracy = accuracy_score(y_train_binary, y_pred_train_binary_svm)

# Calculate accuracy on the testing set
testing_accuracy = accuracy_score(y_test_binary, y_pred_test_binary_svm)

print(f"Training Set Accuracy: {training_accuracy}")
print(f"Testing Set Accuracy: {testing_accuracy}")

from sklearn.neighbors import LocalOutlierFactor

# Initialize the LOF model
lof_model = LocalOutlierFactor(contamination='auto')

# Fit the model to the training data and predict anomalies
lof_model.fit(X_train_processed)
y_pred_train_lof = lof_model.fit_predict(X_train_processed)
y_pred_test_lof = lof_model.fit_predict(X_test_processed)

# Convert anomaly predictions to binary labels (0 for inliers, 1 for outliers)
y_pred_train_binary_lof = [1 if x == -1 else 0 for x in y_pred_train_lof]
y_pred_test_binary_lof = [1 if x == -1 else 0 for x in y_pred_test_lof]

# Evaluate the model
print("Training Set Evaluation (LOF):")
print(classification_report(y_train_binary, y_pred_train_binary_lof))

print("Testing Set Evaluation (LOF):")
print(classification_report(y_test_binary, y_pred_test_binary_lof))

from sklearn.metrics import accuracy_score

# Convert labels to binary encoding
y_train_binary = [1 if label == 'Current' else 0 for label in y_train]
y_test_binary = [1 if label == 'Current' else 0 for label in y_test]

# Calculate accuracy on training set
train_accuracy = accuracy_score(y_train_binary, y_pred_train)
print("Training Set Accuracy:", train_accuracy)

# Calculate accuracy on testing set
test_accuracy = accuracy_score(y_test_binary, y_pred_test)
print("Testing Set Accuracy:", test_accuracy)

from sklearn.model_selection import train_test_split

# Define features (X) and labels (y)
X = data[['Transaction_Amount', 'Transaction_Volume', 'Average_Transaction_Amount',
               'Frequency_of_Transactions', 'Time_Since_Last_Transaction', 'Age',
               'Income', 'Amount_Ratio']]
y = data['Account_Type']  # Assuming 'Account_Type' is the target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check the shape of the training and testing sets
print("Training set shape:", X_train.shape, y_train.shape)
print("Testing set shape:", X_test.shape, y_test.shape)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Assuming you have your features stored in X and labels stored in y
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest classifier
clf = RandomForestClassifier()

# Train the classifier
clf.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = clf.predict(X_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

from sklearn.svm import SVC
from sklearn.metrics import classification_report

# Initialize the SVM classifier
svm_clf = SVC()

# Train the classifier
svm_clf.fit(X_train, y_train)

# Make predictions on the testing set
svm_y_pred = svm_clf.predict(X_test)

# Evaluate the model
print("Support Vector Machine Classification Report:")
print(classification_report(y_test, svm_y_pred))

from sklearn.metrics import accuracy_score

# Calculate accuracy
svm_accuracy = accuracy_score(y_test, svm_y_pred)
print("Support Vector Machine Accuracy:", svm_accuracy)

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Initialize the Gradient Boosting Classifier
gb_classifier = GradientBoostingClassifier()

# Train the classifier
gb_classifier.fit(X_train, y_train)

# Make predictions on the testing set
gb_y_pred = gb_classifier.predict(X_test)

# Calculate accuracy
gb_accuracy = accuracy_score(y_test, gb_y_pred)
print("Gradient Boosting Classifier Accuracy:", gb_accuracy)

from sklearn.metrics import classification_report

# Generate classification report
gb_classification_report = classification_report(y_test, gb_y_pred)

# Print classification report
print("Gradient Boosting Classifier Classification Report:")
print(gb_classification_report)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.neighbors import LocalOutlierFactor

# Generate synthetic data
X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define base anomaly detection algorithms
anomaly_algorithms = [
    ('Isolation Forest', IsolationForest(contamination='auto', random_state=42)),
    ('Local Outlier Factor', LocalOutlierFactor(contamination='auto')),
]

# Train base anomaly detection algorithms
for name, model in anomaly_algorithms:
    model.fit(X_train)

# Extract anomaly scores from base algorithms
anomaly_scores_train = {name: model.fit_predict(X_train) for name, model in anomaly_algorithms}
anomaly_scores_test = {name: model.fit_predict(X_test) for name, model in anomaly_algorithms}

# Use ensemble methods like Random Forest to combine anomaly scores
ensemble_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Prepare the training data for the ensemble classifier
X_train_ensemble = np.column_stack(list(anomaly_scores_train.values()))

# Train the ensemble classifier
ensemble_classifier.fit(X_train_ensemble, y_train)

# Prepare the testing data for the ensemble classifier
X_test_ensemble = np.column_stack(list(anomaly_scores_test.values()))

# Make predictions using the ensemble classifier
y_pred_ensemble = ensemble_classifier.predict(X_test_ensemble)

# Evaluate the ensemble classifier
f1 = f1_score(y_test, y_pred_ensemble)
print("F1 Score (Ensemble):", f1)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import IsolationForest

# Assuming you have already defined model names
model_names_extended = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting', 'K-Nearest Neighbors']

# Define empty dictionaries to store results
results_hyper = {'Train AUC (Hyper)': [], 'Test AUC (Hyper)': []}
results_normal = {'Train AUC (Normal)': [], 'Test AUC (Normal)': []}

# Populate dictionaries with results
for model_name in model_names_extended:
    if model_name == 'Logistic Regression':
        model = LogisticRegression(multi_class='ovr')
    elif model_name == 'Decision Tree':
        model = DecisionTreeClassifier()
    elif model_name == 'Random Forest':
        model = RandomForestClassifier()
    elif model_name == 'Gradient Boosting':
        model = GradientBoostingClassifier()
    elif model_name == 'K-Nearest Neighbors':
        model = KNeighborsClassifier()
    elif model_name == 'Isolation Forest':
        model = IsolationForest()

    # Train normally implemented model
    model.fit(X_train, y_train)

    # Evaluate normally implemented model
    if hasattr(model, "decision_function"):
        y_scores_train_normal = model.decision_function(X_train)
        y_scores_test_normal = model.decision_function(X_test)
    else:
        y_scores_train_normal = model.predict_proba(X_train)[:,1]
        y_scores_test_normal = model.predict_proba(X_test)[:,1]

    auc_train_normal = roc_auc_score(y_train, y_scores_train_normal)
    auc_test_normal = roc_auc_score(y_test, y_scores_test_normal)

    results_normal['Train AUC (Normal)'].append(auc_train_normal)
    results_normal['Test AUC (Normal)'].append(auc_test_normal)

# Convert dictionaries to DataFrames
df_normal = pd.DataFrame(results_normal, index=model_names_extended)

# Print AUC values
print("\nAUC values for Normally Implemented Models:")
print(df_normal)

from sklearn.preprocessing import LabelEncoder

# Assuming y_train and y_test are your categorical labels
label_encoder = LabelEncoder()
y_train_binary = label_encoder.fit_transform(y_train)
y_test_binary = label_encoder.transform(y_test)

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import LabelEncoder

# Define the models
models = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'K-Nearest Neighbors': KNeighborsClassifier(),
    'Isolation Forest': IsolationForest()
}

# Encode categorical labels into binary format
label_encoder = LabelEncoder()
y_train_binary = label_encoder.fit_transform(y_train)
y_test_binary = label_encoder.transform(y_test)

# Plot ROC curves for normally implemented models
plt.figure(figsize=(12, 6))
for model_name in model_names_extended:
    model = models[model_name]
    model.fit(X_train, y_train)

    if hasattr(model, "decision_function"):
        y_scores = model.decision_function(X_test)
    else:
        y_scores = model.predict_proba(X_test)[:, 1]

    fpr, tpr, _ = roc_curve(y_test_binary, y_scores)
    auc = roc_auc_score(y_test_binary, y_scores)

    plt.plot(fpr, tpr, label=f"{model_name} (AUC = {auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Normally Implemented Models')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.model_selection import GridSearchCV

# Define the hyperparameters grid for each model
param_grid_lr = {'C': [0.001, 0.01, 0.1, 1, 10, 100],
                 'penalty': ['l1', 'l2']}
param_grid_dt = {'max_depth': [None, 5, 10, 15, 20],
                 'min_samples_split': [2, 5, 10],
                 'min_samples_leaf': [1, 2, 4]}
param_grid_rf = {'n_estimators': [100, 200, 300],
                 'max_depth': [None, 5, 10, 15],
                 'min_samples_split': [2, 5, 10],
                 'min_samples_leaf': [1, 2, 4]}
param_grid_gb = {'n_estimators': [100, 200, 300],
                 'learning_rate': [0.1, 0.05, 0.01],
                 'max_depth': [3, 4, 5]}
param_grid_knn = {'n_neighbors': [3, 5, 7, 9],
                  'weights': ['uniform', 'distance']}
param_grid_if = {'n_estimators': [100, 200, 300],
                 'max_samples': ['auto', 100, 200, 300],
                 'contamination': [0.1, 0.2, 0.3]}

# Define grid search objects for each model
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='roc_auc')
grid_search_dt = GridSearchCV(DecisionTreeClassifier(), param_grid_dt, cv=5, scoring='roc_auc')
grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5, scoring='roc_auc')
grid_search_gb = GridSearchCV(GradientBoostingClassifier(), param_grid_gb, cv=5, scoring='roc_auc')
grid_search_knn = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, scoring='roc_auc')
grid_search_if = GridSearchCV(IsolationForest(), param_grid_if, cv=5, scoring='roc_auc')

# Store grid search objects in a list
grid_searches_extended = [grid_search_lr, grid_search_dt, grid_search_rf, grid_search_gb, grid_search_knn, grid_search_if]

# Define empty dictionaries to store results
results_hyper = {'Train AUC (Hyper)': [], 'Test AUC (Hyper)': []}

# Populate dictionaries with results
for grid_search, model_name in zip(grid_searches_extended, model_names_extended):
    grid_search.fit(X_train, y_train)
    best_model = grid_search.best_estimator_
    best_model.fit(X_train, y_train)

    # Evaluate the model
    y_scores_train = best_model.predict_proba(X_train)[:, 1]
    y_scores_test = best_model.predict_proba(X_test)[:, 1]

    auc_train = roc_auc_score(y_train, y_scores_train)
    auc_test = roc_auc_score(y_test, y_scores_test)

    results_hyper['Train AUC (Hyper)'].append(auc_train)
    results_hyper['Test AUC (Hyper)'].append(auc_test)

# Convert dictionary to DataFrame
df_hyper = pd.DataFrame(results_hyper, index=model_names_extended)

# Print AUC values
print("AUC values for Models after Hyperparameter Tuning:")
print(df_hyper)

import pandas as pd
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# Define the models
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(),
    'Lasso Regression': Lasso(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor()
    # Add other regression models here
}

# Train each model and collect results
results = []
for model_name, model in models.items():
    model.fit(X_train, y_train)  # Train the model

    # Predict on test data
    y_pred = model.predict(X_test)

    # Calculate mean squared error
    mse = mean_squared_error(y_test, y_pred)

    # Extract coefficients and intercepts if applicable
    if isinstance(model, (LinearRegression, Ridge, Lasso)):
        coefficients = model.coef_
        intercept = model.intercept_
    else:
        coefficients = None
        intercept = None

    # Store results
    results.append({
        'Model': model_name,
        'Mean Squared Error': mse,
        'Coefficients': coefficients,
        'Intercept': intercept
    })

# Create DataFrame from results
results_df = pd.DataFrame(results)

# Display the results table
print(results_df)

import torch
import torch.nn as nn
import torch.optim as optim
from torch_geometric.nn import GCNConv
from torch_geometric.data import Data
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
import numpy as np

# Define the GNN model
class GNN(nn.Module):
    def __init__(self, num_features, hidden_size, output_size):
        super(GNN, self).__init__()
        self.conv1 = GCNConv(num_features, hidden_size)
        self.conv2 = GCNConv(hidden_size, output_size)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = self.conv2(x, edge_index)
        return x

# Adjust input_size, hidden_size, and output_size according to your data
model = GNN(num_features=10, hidden_size=16, output_size=1)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training
def train_model(data, model, criterion, optimizer, num_epochs=100):
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(data)
        loss = criterion(outputs.squeeze(), data.y)
        loss.backward()
        optimizer.step()

# Evaluation
def evaluate_model(data, model):
    model.eval()
    with torch.no_grad():
        outputs = model(data)
        y_pred = outputs.squeeze().numpy()
        y_true = data.y.numpy()

    return y_true, y_pred

# Placeholder data for demonstration
X = torch.randn(200, 10)  # Replace with your actual data
edge_index = torch.tensor([[0, 1, 2], [1, 2, 3]], dtype=torch.long)  # Replace with your actual data
y = torch.randn(200, 1)  # Replace with your actual data

data = Data(x=X, edge_index=edge_index, y=y)

# Train the model
train_model(data, model, criterion, optimizer)

# Evaluate the model
y_true, y_pred = evaluate_model(data, model)

mse = mean_squared_error(y_true, y_pred)
mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mse)
mape = mean_absolute_percentage_error(y_true, y_pred) * 100

print("Mean Squared Error (MSE):", mse)
print("Mean Absolute Error (MAE):", mae)
print("Root Mean Squared Error (RMSE):", rmse)
print("Mean Absolute Percentage Error (MAPE):", mape)

import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error
import numpy as np

# Define the RNN model
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size, num_layers=1):
        super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        h0 = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size).to(x.device)
        out, _ = self.rnn(x, h0)
        out = self.fc(out[:, -1, :])
        return out

# Placeholder data for demonstration
X = torch.randn(200, 10, 1)  # Replace with your actual data
y = torch.randn(200, 1)       # Replace with your actual data

# Define model, loss function, and optimizer
model = SimpleRNN(input_size=1, hidden_size=64, output_size=1)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training
def train_model(X, y, model, criterion, optimizer, num_epochs=100):
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X)
        loss = criterion(outputs.squeeze(), y)
        loss.backward()
        optimizer.step()

# Evaluation
def evaluate_model(X, y, model):
    model.eval()
    with torch.no_grad():
        outputs = model(X)
        y_pred = outputs.squeeze().numpy()
        y_true = y.numpy()

    return y_true, y_pred

# Train the model
train_model(X, y, model, criterion, optimizer)

# Evaluate the model
y_true, y_pred = evaluate_model(X, y, model)

mse = mean_squared_error(y_true, y_pred)
mae = mean_absolute_error(y_true, y_pred)
rmse = np.sqrt(mse)
mape = mean_absolute_percentage_error(y_true, y_pred) * 100

print("Mean Squared Error (MSE):", mse)
print("Mean Absolute Error (MAE):", mae)
print("Root Mean Squared Error (RMSE):", rmse)
print("Mean Absolute Percentage Error (MAPE):", mape)

import matplotlib.pyplot as plt
import numpy as np

# Model names
models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'Gradient Boosting',
          'K-Nearest Neighbors', 'LOF', 'GNN', 'RNN']

# Accuracy values
train_accuracy = [0.48, 0.49, 0.49375, 0.52, 0.52, 0.52, 0.5, 0.5]
test_accuracy = [0.5, 0.46, 0.455, 0.5, 0.5, 0.49, 0.5, 0.5]

# F1-score values
f1_score_train = [0.31, 0.49, 0.49375, 0.36, 0.59, 0.36, 0.52, 0.5]
f1_score_test = [0.33, 0.45, 0.455, 0.34, 0.55, 0.34, 0.52, 0.5]

x = np.arange(len(models))

# Plotting accuracy
plt.figure(figsize=(16, 6))
plt.subplot(1, 2, 1)
plt.bar(x - 0.2, train_accuracy, width=0.4, label='Training Accuracy')
plt.bar(x + 0.2, test_accuracy, width=0.4, label='Testing Accuracy')
plt.xticks(x, models, rotation=45)
plt.ylabel('Accuracy')
plt.title('Model Accuracy Comparison')
plt.legend()

# Plotting F1-score
plt.subplot(1, 2, 2)
plt.bar(x - 0.2, f1_score_train, width=0.4, label='Training F1-score')
plt.bar(x + 0.2, f1_score_test, width=0.4, label='Testing F1-score')
plt.xticks(x, models, rotation=45)
plt.ylabel('F1-score')
plt.title('Model F1-score Comparison')
plt.legend()

plt.tight_layout()
plt.show()

